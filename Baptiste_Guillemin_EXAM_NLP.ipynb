{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 1"
      ],
      "metadata": {
        "id": "TxQcovkd1iV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)"
      ],
      "metadata": {
        "id": "WxQ64eHs1npP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YMQkNdsi1Svj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    text = re.sub('[^A-Za-z]+', ' ', text) #only dalphanumeric characters\n",
        "    text = re.sub('\\W', ' ', text) \n",
        "    text = re.sub('\\s+', ' ', text) #White spaces\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is an english text test      with spaces and too much ponctuation or whatever ,:: @@#\"\n",
        "clean_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MFxdCkp79sTR",
        "outputId": "cd2f640e-65a5-44a3-b443-23ba7e7fb246"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is an english text test with spaces and too much ponctuation or whatever '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) STopwords"
      ],
      "metadata": {
        "id": "q8uYO1ad2jFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "tokens = ['This', 'is', 'a', 'sentence', 'with', 'stopwords', 'that', 'we', 'want', 'to', 'remove']\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PRgXOs62kQx",
        "outputId": "2307927c-416d-4fe8-c59d-e986e3d0959d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'sentence', 'stopwords', 'want', 'remove']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Tokenize"
      ],
      "metadata": {
        "id": "trgfd6Zl2-dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "#defining function for tokenization\n",
        "def tokenization(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return tokens\n",
        "    \n",
        "#applying function to the column\n",
        "data['tokenized']= data['clean'].apply(lambda x: tokenization(x))"
      ],
      "metadata": {
        "id": "x7NHOGZl3BLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')"
      ],
      "metadata": {
        "id": "2o1PR7Wh8fqt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Lemmatize\n"
      ],
      "metadata": {
        "id": "E92u3LTG3SOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "  lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "  return lemmatized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvUlklL83Uav",
        "outputId": "ce62269b-35b5-4268-d6fa-d65bdcfebccb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"groups :\"\n",
        "lemmatized_text = lemmatize_text(input_text)\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_3r06z94S5X",
        "outputId": "b33f0acd-9525-4e1c-b9c0-e611a4932f0a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "group :\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercie 2"
      ],
      "metadata": {
        "id": "tswcu_HH5sVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def extract_nouns(text):\n",
        "  # Tokenize the text\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  # Part-of-speech tag the tokens\n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "  # Filter the pos_tags to include only nouns\n",
        "  nouns = [word for word, pos in pos_tags if pos == 'NN']\n",
        "  return nouns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSy5Gh_E4gfv",
        "outputId": "f8d9a911-076b-41f8-8cb6-85d56a80cc02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"This is a code that extract nouns from a text\"\n",
        "nouns = extract_nouns(input_text)\n",
        "print(nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTdIj0k15x3E",
        "outputId": "9f337ccb-9c7d-4417-d25d-395c69997a37"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['code', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) extract usernames from email"
      ],
      "metadata": {
        "id": "y9iOzNxa6-gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(email_address):\n",
        "    data = email_address.split(\"@\")[0]\n",
        "    result = \"\"\n",
        "    for i in data:\n",
        "        if i.isalpha():\n",
        "            result+=i\n",
        "    return result\n",
        "\n",
        "email_address = \"john@example.com\"\n",
        "print(\"Original Email:\", email_address)\n",
        "print(\"Extract the name from the said Email address:\")\n",
        "print(test(email_address))\n",
        "email_address = \"john.smith@example.com\"\n",
        "print(\"\\nOriginal Email:\", email_address)\n",
        "print(\"Extract the name from the said Email address:\")\n",
        "print(test(email_address))\n",
        "email_address = \"disposable.style.email.with+symbol@example.com@example.com\"\n",
        "print(\"\\nOriginal Email:\", email_address)\n",
        "print(\"Extract the name from the said Email address:\")\n",
        "print(test(email_address))\n",
        "email_address = \"fully-qualified-domain@example.com\"\n",
        "print(\"\\nOriginal Email:\", email_address)\n",
        "print(\"Extract the name from the said Email address:\")\n",
        "print(test(email_address))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_aBezqA5zmw",
        "outputId": "22490935-471c-4fb8-d6bd-09d6d2549a2a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Email: john@example.com\n",
            "Extract the name from the said Email address:\n",
            "john\n",
            "\n",
            "Original Email: john.smith@example.com\n",
            "Extract the name from the said Email address:\n",
            "johnsmith\n",
            "\n",
            "Original Email: disposable.style.email.with+symbol@example.com@example.com\n",
            "Extract the name from the said Email address:\n",
            "disposablestyleemailwithsymbol\n",
            "\n",
            "Original Email: fully-qualified-domain@example.com\n",
            "Extract the name from the said Email address:\n",
            "fullyqualifieddomain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_usernames(text):\n",
        "  # Use a regular expression to find all email addresses in the text\n",
        "  email_pattern = r'[\\w\\.-]+@[\\w\\.-]+'\n",
        "  emails = re.findall(email_pattern, text)\n",
        "  # Extract the username from each email address\n",
        "  usernames = [email.split('@')[0] for email in emails]\n",
        "  return usernames"
      ],
      "metadata": {
        "id": "Al3uOWmX7DAm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Please contact me at AlainDeloin@example.com or ginette@example.com for more information.\"\n",
        "usernames = extract_usernames(input_text)\n",
        "print(usernames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzo77OBz7q7V",
        "outputId": "6b8db20c-da0a-4fa0-cf98-05144e003052"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AlainDeloin', 'ginette']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Om4QCGv7v6J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}