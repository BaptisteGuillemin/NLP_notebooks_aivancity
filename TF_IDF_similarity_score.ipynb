{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 1:"
      ],
      "metadata": {
        "id": "lQVa2KUT81Y5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSqq0w4q5yWr",
        "outputId": "6273de26-3e04-4aac-c68d-b10e8f99e8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import math\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of documents to process\n",
        "docs = [\"A document is a written, printed, or electronic text.\",\n",
        "        \"A document is a set of written or printed pages.\",\n",
        "        \"A document is a written or printed statement of facts.\",\n",
        "        \"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.\"]\n"
      ],
      "metadata": {
        "id": "HOGcKDO87R3E"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to calculate the TF-IDF for a given word in a document\n",
        "def tfidf(word, doc, docs):\n",
        "    tf = doc.count(word) / len(doc)\n",
        "    idf = math.log(len(docs) / sum([1 for d in docs if word in d]))\n",
        "    return tf * idf\n",
        "\n",
        "# Define a function to find the 5 most important keywords in a document\n",
        "def keywords(doc, docs):\n",
        "    words = [w for w in doc if w not in stop_words]\n",
        "    scores = {w: tfidf(w, doc, docs) for w in words}\n",
        "    sorted_words = sorted(scores, key=scores.get, reverse=True)\n",
        "    return sorted_words[:5]\n",
        "\n",
        "# Tokenize the documents and remove stop words\n",
        "docs = [nltk.word_tokenize(d) for d in docs]\n",
        "docs = [[w for w in d if w not in stop_words] for d in docs]\n",
        "\n",
        "# Calculate the TF-IDF for each word in each document\n",
        "tfidf_scores = [[tfidf(w, d, docs) for w in d] for d in docs]\n",
        "\n",
        "# Find the 5 most important keywords in each document\n",
        "keywords = [keywords(d, docs) for d in docs]\n",
        "\n",
        "# Print the results\n",
        "for i, doc in enumerate(docs):\n",
        "    print(\"Document {}:\".format(i + 1))\n",
        "    print(\"Total words: {}\".format(len(doc)))\n",
        "    print(\"Number of sentences: {}\".format(doc.count('.')))\n",
        "    print(\"Keywords: {}\".format(keywords[i]))\n",
        "        # Print the TF-IDF scores of the five most important words\n",
        "    for word, score in zip(keywords[i], tfidf_scores[i]):\n",
        "        print(\"{}: {}\".format(word, score))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwhVbJWH6O5Q",
        "outputId": "ba176286-62f6-4208-cc2d-7aa4ba06ce5d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "Total words: 9\n",
            "Number of sentences: 1\n",
            "Keywords: [',', 'electronic', 'text', 'written', 'printed']\n",
            ",: 0.0\n",
            "electronic: 0.0\n",
            "text: 0.03196467471686454\n",
            "written: 0.15403270679109896\n",
            "printed: 0.03196467471686454\n",
            "Document 2:\n",
            "Total words: 7\n",
            "Number of sentences: 1\n",
            "Keywords: ['set', 'pages', 'written', 'printed', 'A']\n",
            "set: 0.0\n",
            "pages: 0.0\n",
            "written: 0.19804205158855578\n",
            "printed: 0.04109743892168297\n",
            "A: 0.04109743892168297\n",
            "Document 3:\n",
            "Total words: 7\n",
            "Number of sentences: 1\n",
            "Keywords: ['statement', 'facts', 'written', 'printed', 'A']\n",
            "statement: 0.0\n",
            "facts: 0.0\n",
            "written: 0.04109743892168297\n",
            "printed: 0.04109743892168297\n",
            "A: 0.19804205158855578\n",
            "Document 4:\n",
            "Total words: 150\n",
            "Number of sentences: 8\n",
            "Keywords: ['tf–idf', ',', 'word', 'used', 'ranking']\n",
            "tf–idf: 0.009241962407465937\n",
            ",: 0.018483924814931874\n",
            "word: 0.018483924814931874\n",
            "used: 0.046209812037329684\n",
            "ranking: 0.06469373685226157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 2:"
      ],
      "metadata": {
        "id": "5K4Kazqt87fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.\"\n",
        "text2 =  \"In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\""
      ],
      "metadata": {
        "id": "EhjqIi3g-t0j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Vectorize the texts using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "# Create a matrix from the vectorized texts\n",
        "matrix = vectors.todense()\n",
        "\n",
        "# Calculate the semantic similarity of the texts using cosine similarity\n",
        "similarity_score = cosine_similarity(matrix)[0,1]\n",
        "\n",
        "# Print the similarity score\n",
        "print(similarity_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiYTxYA6bPs",
        "outputId": "4ccbc9e2-b74f-4218-9b86-232a07585cab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8526932516641659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BsWXl_SHAFs9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}